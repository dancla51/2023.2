---
title: "Assignment1_DC"
author: "Daniel Clark"
date: "2023-08-01"
output: html_document
---

```{r setup, message = F, warning = F}
library(tidyverse)
library(ggplot2)
```

## Task 1 - Revision

This is revision for ENGSCI314:

What is the correct interpretation of a P-value, as given in the results of a statistical test?

a) The P-value is the probability that we have evidence to support our null hypothesis
b) The P-value is the probability that we have evidence against our null hypothesis
c) The P-value is the probability that, if the null hypothesis is FALSE, sampling variation would
   produce a sample that is at least as unusual as our sample.
d) The P-value is the probability that, if the null hypothesis is TRUE, sampling variation would
   produce a sample that is at least as unusual as our sample.


The correct answer is answer d). This is the correct answer as it accurately describes the definition of a P-value in statistics. The other three answers (a, b and c) are incorrect, because they do not correctly describe the definition of a P-value. c) is clearly wrong because it is the opposite of the correct answer, while a) and b) do not make sense because we do not have a probability of evidence against our null hypothesis, we simply have more or less evidence against it.

**BTW, I created this question before you did this in the lecture, I didn't just copy it!**


## Task 2 - Algorithmic Fairness Audit

Subtask 1 - Load in and join together the three data sets (tidyly)

```{r, message=F}
# Load in data
phase1 = read_csv("2022-phase1-new-grad-applicants.csv", show_col_types = FALSE)
phase2 = read_csv("2022-phase2-new-grad-applicants.csv", show_col_types = FALSE)
phase3 = read_csv("2022-phase3-new-grad-applicants.csv", show_col_types = FALSE)
phase_final = read_csv("2022-final-hires-newgrad.csv")

# Convert phase3 df to tidy data
phase3 = as.data.frame(t(phase3)[-1,])
phase3$applicant_id = as.numeric(rownames(phase3))
phase3 = rename(phase3, interviewer_1 = V1)
phase3 = rename(phase3, interviewer_2 = V2)

# Create columns for if they passed the previous phase, assign 1s for yes's
phase2$passed_phase_1 = 1
phase3$passed_phase_2 = 1
phase_final$passed_phase_3 = 1

# Join all together
all_phases = phase1 %>% left_join(phase2) %>% left_join(phase3) %>% left_join(phase_final)

# Replace NAs in the passed phase columns with 0's (leave others as is)
all_phases$passed_phase_1[is.na(all_phases$passed_phase_1)] <- 0
all_phases$passed_phase_2[is.na(all_phases$passed_phase_2)] <- 0
all_phases$passed_phase_3[is.na(all_phases$passed_phase_3)] <- 0 

# I chose to leave these indicators as binary, to make calculations easier later on

# View
knitr::kable(head(all_phases, 10))
```

Subtask 2 - Summary, Statistical Test, and Chart

Appropriate Numeric Summary:

```{r}
# Test the proportion of applicants that passed phase 1, 2, and 3 for each team
team_proportions = as.data.frame(t(group_by(all_phases, team_applied_for) %>% summarise(mean(passed_phase_1), mean(passed_phase_2), mean(passed_phase_3)))[-1,])
# Rename columns
team_proportions = rename(team_proportions, data_team = V1)
team_proportions = rename(team_proportions, software_team = V2)

team_proportions$ratio_datatosoftware = as.numeric(team_proportions$data_team) / as.numeric(team_proportions$software_team)
```

As we can see from this summary, at each stage of the process, the applicants for the data team have slightly higher pass rates than the applicants for the software team. This could be because there are more applicants for the software team, despite there being the same amount of roles available, or it could be an inherent bias in the system to applicants with more of a data-background than a software-background.

I also created a ratio column to express this difference more explicitly. From this column we can see that this *potential bias* gets worse the further along the application process the applicants get.

Now I will investigate the reason for this discrepancy:
```{r}
# Find how many applicants for each team
num_data_ap = sum(all_phases$team_applied_for=="Data")
num_software_ap = sum(all_phases$team_applied_for=="Software")

```

The number of data applicants is `r num_data_ap`.
The number of software applicants is `r num_software_ap`

It seems that the reason data applicants get accepted through to future phases at a higher rate is because there are fewer applicants for the data team, compare to the software team. Assuming there are a similar number of roles available in each team, this would force the algorithm to be *more harsh* to software team applicants than data team applicants. 

I believe that Black Saber hiring team should advertise the Data team more, to encourage more applicants for the data team, so that all applicants can be treated equally through the hiring process.



Basic statistical tests (think t-test or ANOVA/F-test)

```{r}
# Test leadership skills based on GPA and gender
skills_fit = lm(leadership_presence ~ gpa+gender, data=all_phases)
anova(skills_fit)

```

From this ANOVA test, we can see the effects of both GPA and gender on the rating given to that applicant's *'leadership presence'*. Based on the results, we can see that the effects of GPA are statistically insignificant, as the P-value for this test is easily greater than 0.05. However, concerningly, the effects of gender are statistically *significant*, as the P-value is well below 0.05. This means that the AI seems to be rating applicant's leadership_presence based on their gender.

Here I take summary results to see more information:

```{r}
summary(skills_fit)
```

We can see from these summary results that there is no significant difference between the AI's gradings between 'man' and 'prefer not to say', but that there IS a significant difference between 'man' and 'woman'. The P-value of 0.001 makes it a very significant difference. Now I will plot these results to visualise it graphically:

```{r, warning=F}
ggplot(all_phases, aes(x=gender, y=leadership_presence), col=factor(passed_phase_2)) + geom_boxplot() + geom_jitter(aes(col=factor(passed_phase_2)))
```

This Box and Whisker plot clearly shows the disparity in scores between men and women (As there are only 3 observations in the prefer not to say category, we cannot make conclusions on how the AI treats this gender). The women that applied appear to have higher frequencies throughout the lower and middle ranges of the scoring, meanwhile there are far more men represented at the highest levels (>8).

I have also coloured the points by whether or not the applicant passed this phase (phase 2) of the application process. From this we can see that it tends to be applicants with a leadership score of >5 that progress to the next phase. Because the applicants with a score >5 are mostly men, this greatly advantages men in the application process.

```{r}
pr = group_by(all_phases, gender) %>% summarise(sum(passed_phase_1), sum(passed_phase_2))
knitr::kable(pr)
```

This is further proven by calculating how many men and women passed through stage two of the process. There were more than twice as many men as women that passed this stage, despite the fact that there were more women than men that made it through phase 1! This seems to prove that the AI is biased against women in it's leadership presence score, and possibly other ways, which result in it accepting twice as many men as women.


**Subtask 3 - Alt Text for my chart**

"Box-and-whisker plot of Leadership Scores given by recruitment AI, coloured by applicant pass-rate, of Black Saber graduate applicants, showing potential gender bias in AI system."



