---
title: "Assignment1_DC"
author: "Daniel Clark"
date: "2023-08-01"
output: html_document
---

```{r setup, message = F, warning = F}
library(tidyverse)
library(ggplot2)
```

## Task 1 - Revision

This is revision for ENGSCI314:

What is the correct interpretation of a P-value, as given in the results of a statistical test?

a) The P-value is the probability that we have evidence to support our null hypothesis
b) The P-value is the probability that we have evidence against our null hypothesis
c) The P-value is the probability that, if the null hypothesis is FALSE, sampling variation would
   produce a sample that is at least as unusual as our sample.
d) The P-value is the probability that, if the null hypothesis is TRUE, sampling variation would
   produce a sample that is at least as unusual as our sample.


The correct answer is answer d). This is the correct answer as it accurately describes the definition of a P-value in statistics. The other three answers (a, b and c) are incorrect, because they do not correctly describe the definition of a P-value. c) is clearly wrong because it is the opposite of the correct answer, while a) and b) do not make sense because we do not have a probability of evidence against our null hypothesis, we simply have more or less evidence against it.

**BTW, I created this question before you did this in the lecture, I promise!**


## Task 2 - Algorithmic Fairness Audit

Subtask 1 - Load in and join together the three data sets (tidyly)

```{r, message=F}
# Load in data
phase1 = read_csv("2022-phase1-new-grad-applicants.csv", show_col_types = FALSE)
phase2 = read_csv("2022-phase2-new-grad-applicants.csv", show_col_types = FALSE)
phase3 = read_csv("2022-phase3-new-grad-applicants.csv", show_col_types = FALSE)
phase_final = read_csv("2022-final-hires-newgrad.csv")

# Convert phase3 df to tidy data
phase3 = as.data.frame(t(phase3)[-1,])
phase3$applicant_id = as.numeric(rownames(phase3))
phase3 = rename(phase3, interviewer_1 = V1)
phase3 = rename(phase3, interviewer_2 = V2)

# Create columns for if they passed the previous phase, assign 1s for yes's
phase2$passed_phase_1 = 1
phase3$passed_phase_2 = 1
phase_final$passed_phase_3 = 1

# Join all together
all_phases = phase1 %>% left_join(phase2) %>% left_join(phase3) %>% left_join(phase_final)

# Replace NAs in the passed phase columns with 0's (leave others as is)
all_phases$passed_phase_1[is.na(all_phases$passed_phase_1)] <- 0
all_phases$passed_phase_2[is.na(all_phases$passed_phase_2)] <- 0
all_phases$passed_phase_3[is.na(all_phases$passed_phase_3)] <- 0 

# I chose to leave these indicators as binary, to make calculations easier later on

# View
knitr::kable(head(all_phases, 10), align="c")
```

Subtask 2 - Summary, Statistical Test, and Chart

Appropriate Numeric Summary:

```{r}
# Test the proportion of applicants that passed phase 1, 2, and 3 for each team
team_proportions = as.data.frame(t(group_by(all_phases, team_applied_for) %>% summarise(mean(passed_phase_1), mean(passed_phase_2), mean(passed_phase_3)))[-1,])
# Rename columns
team_proportions = rename(team_proportions, data_team = V1)
team_proportions = rename(team_proportions, software_team = V2)

team_proportions$ratio_datatosoftware = as.numeric(team_proportions$data_team) / as.numeric(team_proportions$software_team)

knitr::kable(team_proportions, align="c")
```

As we can see from this summary, at each stage of the process, the applicants for the data team have slightly higher pass rates than the applicants for the software team. This could be because there are more applicants for the software team, despite there being the same amount of roles available, or it could be an inherent bias in the system to applicants with more of a data-background than a software-background.

I also created a ratio column to express this difference more explicitly. From this column we can see that this *potential bias* gets worse the further along the application process the applicants get.

Now I will investigate the reason for this discrepancy:
```{r}
# Find how many applicants for each team
num_data_ap = sum(all_phases$team_applied_for=="Data")
num_software_ap = sum(all_phases$team_applied_for=="Software")

```

The number of data applicants is `r num_data_ap`.
The number of software applicants is `r num_software_ap`

It seems that the reason data applicants get accepted through to future phases at a higher rate is because there are fewer applicants for the data team, compare to the software team. Assuming there are a similar number of roles available in each team, this would force the algorithm to be *more harsh* to software team applicants than data team applicants.



Basic statistical tests (think t-test or ANOVA/F-test)

```{r}
# Test leadership skills based on GPA and gender
skills_fit = lm(leadership_presence ~ gpa+gender, data=all_phases)
anova(skills_fit)

```

From this ANOVA test, we can see the effects of both GPA and gender on the rating given to that applicant's *'leadership presence'*. Based on the results, we can see that the effects of GPA are statistically insignificant, as the P-value for this test is easily greater than 0.05. However, concerningly, the effects of gender are statistically *significant*, as the P-value is well below 0.05. This means that the AI seems to be rating applicant's leadership_presence based on their gender.

Here I take summary results to see more information:

```{r}
summary(skills_fit)
```

We can see from these summary results that there is no significant difference between the AI's gradings between 'man' and 'prefer not to say', but that there IS a significant difference between 'man' and 'woman'. The P-value of 0.001 makes it a very significant difference. Now I will plot these results to visualise it graphically:

```{r, warning=F}
ggplot(all_phases, aes(x=gender, y=leadership_presence), col=factor(passed_phase_2)) + geom_boxplot() + geom_jitter(aes(col=factor(passed_phase_2)))
```

This Box and Whisker plot clearly shows the disparity in scores between men and women (As there are only 3 observations in the prefer not to say category, we cannot make conclusions on how the AI treats this gender). The women that applied appear to have higher frequencies throughout the lower and middle ranges of the scoring, meanwhile there are far more men represented at the highest levels (>8).

I have also coloured the points by whether or not the applicant passed this phase (phase 2) of the application process. From this we can see that it tends to be applicants with a leadership score of >5 that progress to the next phase. Because the applicants with a score >5 are mostly men, this greatly advantages men in the application process.

```{r}
pr = group_by(all_phases, gender) %>% summarise(sum(passed_phase_1), sum(passed_phase_2))
knitr::kable(pr, align="c")
```

This is further proven by calculating how many men and women passed through stage two of the process. There were more than twice as many men as women that passed this stage, despite the fact that there were more women than men that made it through phase 1! This seems to prove that the AI is biased against women in it's leadership presence score, and possibly other ways, which result in it accepting twice as many men as women.


**Subtask 3 - Alt Text for my chart**

"Box-and-whisker plot of Leadership Scores given by recruitment AI, coloured by applicant pass-rate, of each gender of Black Saber graduate applicants, showing potential gender bias in AI system."


**Subtask 4 - Short Conclusion for Black Saber:**

Based on the above data analysis, I believe that the Black Saber hiring AI has two problems. 

Firstly, as there are more software applicants than data applicants, the system appears to favour selection of data applicants. Data applicants are 10% more likely to get through phase 1, 16% more likely to get through stage 2, and 39% more likely to be hired. I believe that Black Saber hiring team should advertise the Data team more, to encourage more applicants for the data team, so that all applicants can be treated equally through the hiring process.

Secondly, the algorithm is very biased against women during phase 2. This is shown by 15/145 (10.3%) of men making it through phase 2, while only 7/152 (4.6%) of women progress through this stage. One reason for this is shown in the chart above, which demonstrated how the algorithm grades women significantly lower than men on the 'leadership presence' characteristic. The algorithm tends to grade women 0.88 units lower on average, which is a large value when taken on a scale of 1-10.

I believe to further drill down into the root of the cause, testing should be completed to determine what are the causes of this discrepancy. The same should be done for all of the other AI-graded scores, as well.


**Bonus Task  - Uglyification of graph**

```{r, warning=F}
ggplot(all_phases, aes(x=gender, y=leadership_presence, col=factor(passed_phase_2))) + geom_boxplot() + geom_jitter(aes(col=factor(passed_phase_2))) + theme_void(base_size = 0) + theme(plot.background = element_rect(fill = "purple")) +coord_polar()
```
^ Basically a pie chart!


## Task 3 - Reprex Critique

"Package philosophy = The reprex code:

-Must run and, therefore, should be run by the person posting. No faking it.

-Should be easy for others to digest, so they don’t necessarily have to run it. You are encouraged to include selected bits of output. :scream:

-Should be easy for others to copy + paste + run, if and only if they so choose. Don’t let inclusion of output break executability." [1] *(Package Philosophy section of https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html)*


Bug Report example A is a very poor bug report, based on REPREX values. This is because it is not able to be easily replicated by someone who wishes to examine the bug.This fails the reprex philosophy statement 3 that code *"Should be easy for others to copy + paste + run"*[1]. Someone wishing to run the code will not have `my_url` saved on their computer so this will cause errors. Also they may not have the 'rvest' library loaded which will cause errors. Compared to this, both examples 2 and 3 do succeed at passing this requirement.

Bug Report, by contrast, is much better because it is replicatable and is *"easy for others to copy + paste + run"*[1]. However, it does not meet the reprex philosophy statement 3 that it *"Should be easy for others to digest"*[1]. This is because the output from the link is cluttered, and it is difficult to immediately tell what the issue is, let alone how to fix it.

Finally, bug report C is also close but fails to be a perfect reprex in a different place. It meets requirement 2, as it is easy for others to digest, with only limited output, that highlights the difference between html_text an html_text2. It also meets requirement 3 as it is very easy to copy, paste and then run. (Which I have shown by doing it below.) However it does not meet requirement 1, as the output shown in the bug report is different than what is actually produced when the code is run, so clearly the reporter has not really run the code before posting this.
```{r}
library(rvest)
some_html <- '<p dir="ltr" style="text-align:left;"></p><span style="font-size:0.9375rem;">T
he sentence starts this way,</span><span style="font-size:0.9375rem;"> </span><span
style="font-size:0.9375rem;">then</span><span style="font-size:0.9375rem;"> </span><
span style="font-size:0.9375rem;">spaces</span><span style="font-size:0.9375rem;">
</span><span style="font-size:0.9375rem;">disappear</span>'
html_text(read_html(some_html)) # is correct
#> [1] "The sentence starts this way, then spaces disappear"
html_text2(read_html(some_html)) # not correct
#> [1] "The sentence starts this way,thenspacesdisappear"
```


## Task 4 - Reflection

Graduate Capability #5 at the University of Auckland is described as *"Graduates of the University are expected to be able to learn and work autonomously and ethically. They are expected to be lifelong learners, to show resilience, proactivity and an ability to make principled decisions in academic and professional spheres."* [2] *(Section 4 of https://www.auckland.ac.nz/en/students/forms-policies-and-guidelines/student-policies-and-guidelines/graduate-profile.html)*. This was something we went into in-depth during Task 2 of lab 2 about ethical scraping.

Web scraping is one of the most 'grey' moral areas when programming, so learning how to do this ethically and legally is very important. It was interesting to learn about the robots.txt, and how we can search the company's Terms and Conditions for terms like 'scraping' and 'crawl' to find the relevant sections on whether they allow people to scrape their website. It's also important to respect a site's default crawl delay time, and not overload the website.

In this assignment, I am proud of myself for taking on Liza's feedback about using knitr::kable() and using it in 3 places in this assignment. Although personally I am not sure it looks any better than the default output (in this case), it is a useful skill to learn for displaying larger, more confusing tables.

Something I would do differently for next assignment would be to investigate more attributes of the data. I could have done some initial exploration of the data before jumping into analyses, however this was not asked for in the assignment so for ths assignment I did not.



## References:

[1]  *Jennifer Bryan, Reprex Do's and Dont's, 03/08/2023,  https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html*

[2]  *UoA, Graduate Profiles, 03/08/2023, https://www.auckland.ac.nz/en/students/forms-policies-and-guidelines/student-policies-and-guidelines/graduate-profile.html*


End Submission./
