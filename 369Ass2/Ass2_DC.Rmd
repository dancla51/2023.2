---
title: "Ass2_DC"
author: "Daniel Clark"
date: "2023-08-17"
output: html_document
---

```{r, warning=F, message=F}
library(tidyverse)
library(purrr)
library(ggplot2)
library(leaps)
#library(glmnet)
library(boot)
```



## Task 1.1 - Load data into one file

```{r, eval=F}
# Function to load in CSV file 
load_single_file = function(filename){
  path = paste0("a2_data/",filename)
  df = read_csv(path, show_col_types=FALSE)
  return(df)
}

# List of CSV files
filelist = list.files(path = "a2_data", pattern = "*.csv")
# Map
df_list = filelist %>% map(load_single_file)

# Merge into one DF
spotify_df = df_list[[1]]
for (i in 2:length(df_list)) {
  spotify_df = bind_rows(spotify_df, df_list[[i]])
}

# Save
saveRDS(spotify_df, file="spotify_df")
```


```{r}
spotify_df = readRDS("spotify_df")
# Display
head(spotify_df, 15)
```

**a) How many rows and columns?**
```{r}
size_sum(spotify_df)
```
114,000 rows and 20 columns!


**b) What does each row represent?**

Each row represents one song


**c) How many genres in the dataset?**
```{r}
length(unique(spotify_df$track_genre))
```
There are 114 unique genres!

## Task 1.2

Subtask 1 - Data Visualisation

```{r}
spotify_nonz = filter(spotify_df, popularity>0)
spotify_nonz$duration_min = spotify_nonz$duration_ms / 60 / 1000
# Create Visualisation
b=filter(spotify_nonz, duration_min<=8)
ggplot(b, aes(x=duration_min, y=popularity)) + geom_point(alpha=0.1) + geom_smooth()

```

This visualisation is very interesting because it shows how popular songs tend to be on spotify based on how long they are. The geom_smooth has fit a line to this relationship which shows us that songs tend to be the most popular if they are around 3.5 minutes long.


Subtask 2 - Data cleaning
(`spotify_nonz` has already been modified to remove the 0 popularity scores and has converted song duration to minutes)
```{r}
set.seed(502)


rows_removed = count(spotify_df) - count(spotify_nonz)
```

The number of `rows_removed` when removing observations with a popularity score of 0 was `r rows_removed`

```{r}
# More Cleaning
spotify_clean = spotify_nonz %>% select(popularity:duration_min) %>% select(-c(duration_ms, track_genre, key))

```

Here I have removed all the rows without any meaningful numeric interpretation. For example, track_ID is not going to be a helpful measure for predicting its popularity, and neither is anything else with a huge number of factors, including Artist, Album, Genre, Key etc. I have also removed the duration in milliseconds column, as the duration in minutes attribute already contains this relevant information.

Create linear Model

```{r, cache=T}
# Code from Lab 4
# Crossvalidation code
allyhat<-function(xtrain, ytrain, xtest,lambdas,nvmax=50){
  n<-nrow(xtrain)
  yhat<-matrix(nrow=nrow(xtest),ncol=length(lambdas))
  search<-regsubsets(xtrain,ytrain, nvmax=nvmax, method="back")
  summ<-summary(search)
  for(i in 1:length(lambdas)){
    penMSE<- n*log(summ$rss)+lambdas[i]*(1:nvmax)
    best<-which.min(penMSE)  #lowest penMSE
    betahat<-coef(search, best) #coefficients
    xinmodel<-cbind(1,xtest)[,summ$which[best,]] #predictors in that model
    yhat[,i]<-xinmodel%*%betahat
  }
  yhat
}

# Set y
mf<-model.frame(popularity~.^2, data=spotify_clean)
X<-model.matrix(popularity~.^2, mf)[,-1]
y = spotify_clean$popularity

n<-nrow(X)
folds<-sample(rep(1:10,length.out=n))
lambdas<-c(0,2,4,6,8,10,12)
fitted<-matrix(nrow=n,ncol=length(lambdas))
for(k in 1:10){
  train<- (1:n)[folds!=k]
  test<-(1:n)[folds==k]
  fitted[test,]<-allyhat(X[train,],y[train],X[test,],lambdas)  
}
MSPE = colMeans((y-fitted)^2)
print(MSPE)

fitted = as.data.frame(fitted)
fitted$y_val = y
```


Subtask 3 - Justify Model Selection

I have chosen to select a linear regression model because the `popularity` is a numeric response variable, and we can use linear regression based on the remaining 13 numeric/logical attributes to build a convincing model using linear regression. We can assume independence because each song is unique, and if we wanted to (if Liza had asked us to) we could check other assumptions of this model including checking for equality of variance and cook's distance plot for significant observations.

I have chosen to base this model only on the numeric variables in the spotify data, as the non-numeric variables had too many levels to give us any meaningful insight, and would only have needlessly complicated the data. I was tempted to leave in the 'genre' attribute but decided to remove it because there are still so many levels that it would complicate the model too much for the small possible gain it would give us.

The optimal lambda found by cross-validation is 0. This means that we do not penalise at all having extra attributes in our model, so fitting the final model will give us a model with all parameters that I have left in being active.

I have also chosen not to fit any interaction terms in my model, as this will also greatly increase the time that the model takes to run, as it would effectively square the number of factors involved in the model.

```{r}
# As our optimal lambda value is 0, there is no need to use regsubsets to find different parameter sets.
model = lm(popularity~., data=spotify_clean)

summary(model)$coefficients[,1]

  
```



Subtask 4 - Clearly report your final model


My final model is $Popularity = \beta_0 + \sum_{i=1}^n \beta_{i}X_{i} + \epsilon$, where $\epsilon ^{iid} N(0,\sigma^{2})$, n is the number of explanatory variables = 13, $\beta_{i}$ is the ith estimate as shown in the summary table above, and $X_{i}$ is the value of the corresponding attribute.


## Task 1.3

Choose song from spotify dataset and craft a 95% prediction interval for uncertainty:

```{r}
set.seed(502)
song_num = sample(1:length(spotify_clean$popularity), 1)
song_values = (spotify_clean[song_num,])
knitr::kable(spotify_nonz[song_num,])


```
I have set my seed and then selected a song at random. I don't know the song, but it turns out I do know this artist! This makes me think it might be a fairly popular song. Now to construct a confidence interval.


Bootstrapping

```{r, cache=T}

# Adapted from code in the extra bits by Liza Bolton
predict_spotify = function(data, i){
  model = lm(popularity~., data=spotify_clean[i,])
  predict.lm(model, song_values, interval = "none") + sample(resid(model),1)
}

bootstraps = boot(spotify_clean, predict_spotify, R = 1000)
ci <- bootstraps$t0 + c(-1.96, 1.96) * sd(bootstraps$t)
round(ci, 1)

```

The prediction interval found for the popularity of the song "Hand Clap" by Fitz and the tantrums is the range (18.1-91.3). This is an extremely wide prediction interval showing the imprecision of the model. The true value of popularity for this song is 65, which is within this prediction interval.


## Task 2 - Reflection

1. What is something you’re proud of in the assignment?
  I am proud of the visualisation that I've created in Task 1.2 subtask 2. I think this visualisation is clear, easy to understand, and provides us with helpful information about the relationship between the duration of a song and the popularity of it. the geom_smooth clearly shows that the most popular song-length is 3-4 minutes, which is information that is backed up by the length of songs we tend to hear most commonly on the radio, and in every day life.


2. What are you going to do prepare for the STATS369 midterm AND which topics do you find the most challenging — why? 
  I am going to use past tests to test my knowledge and practice for the upcoming midterm tests. The thing I expect to find the most challenging will be writing R-code in an exam setting because this is not something that I have done before, and I hope to be able to remember enough about the relevant functions.


## References:

https://canvas.auckland.ac.nz/courses/89825/files/11451795?module_item_id=1927869&fd_cookie_set=1, Liza Bolton, accessed 23/08/2023




