---
title: "Ass4_DC"
author: "Daniel Clark"
date: "2023-10-05"
output: html_document
---

```{r setup, message=False, warning=False}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(ggplot2)
library(ranger)
library(glmnet)
library(xgboost)
library(caret)
```

## Task 1

*A*

```{r}
set.seed(502)
```

*B*

```{r}
# Load data
df=read.csv("bank_marketing.csv", sep=";")

```

*C*

The variable 'pdays' should not be considered, as it appears to be coming through with a very large percentage of values at 999, which is unrealistic and is a flaw in the data, so should be removed.

We should also remove the variable 'duration' because, as described in the data dictionary, "last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.". This clearly indicates that it should be removed when creating a predictive model, so that we don't introduce any bias.


## Task 2

*A*

```{r}
# Remove column
df = df %>% mutate(pdays=NULL, duration=NULL)

# Fix up datatypes
df$job = as.factor(df$job)
df$marital = as.factor(df$marital)
df$education = as.factor(df$education)
df$default = as.factor(df$default)
df$contact = as.factor(df$contact)
df$month = as.factor(df$month)
df$day_of_week = as.factor(df$day_of_week)
df$poutcome = as.factor(df$poutcome)

# Code response variable
df$y = as.numeric(ifelse(df$y=="yes", 1, 0))

# Split into training and test
dfyes = filter(df, y==1)
dfno = filter(df, y==0)

numy=count(dfyes)[1,1]
numn=count(dfno)[1,1]
ysamp = sample(numy, numy*0.9)
nsamp = sample(numn,numn*0.9)

train.df = rbind(dfyes[ysamp,], dfno[nsamp,])
test.df = rbind(dfyes[-ysamp,], dfno[-nsamp,])

```

*B and C*

```{r}
# Summary Table of education
df %>% group_by(education) %>% summarise(percent_yes = mean(y)*100)
```

From this visualisation, we can see there is some correlation between education type and the chance of subscribing to a term deposit. However, we do not know if this relationship is causal. We can see that the most likely individuals to subscribe are those with a university degree, or who have not disclosed their education status to the bank. This makes sense as people who have completed university education would tend to earn more and therefore have more disposable income available to invest. Contrastingly, those with only a 'basic' income are far less likely to subscribe for the opposite reason.


```{r}
# Plot
ggplot(data=df, aes(x=age, y=job, col=factor(y))) + geom_jitter(alpha=0.8)

```

From this plot we can see that there is a correlation between someone's job and their likelihood of subscribing. We can tell this because there is a much higher percentage of blue dots for people who are retired, work in admin or management, than for other jobs including being unemployed, a student, a housemaid, or an entrepreneur. We can also see that there is a strong relationship between age and employment status, which indicates age may also be a correlated variable.

```{r}
by_age = group_by(df, age) %>% summarise(m=mean(y))
ggplot(data=by_age, aes(x=age, y=m)) + geom_line()
```

From this graph we can see that the fraction of people who will subscribe to the term deposit appears to be bimodal, with one peak for very young people, quickly dropping down for middle aged (30-60) people, and then increasing to a much higher chance for people 60 years or older.


*D*

I predict that age, education status, and poutcome (whether or not they have subscribed to a term deposit before) will be three of the main predictive variables. The first two I have decided on because of the visualisations, and I chose poutcome because previous behaviour can often indicate future behaviours as well.


## Task 3

*A*

i) Logistic Regression with GLM

```{r}
log_fit = glm(y~., data=train.df)
summary(log_fit)
```

ii) Ridge Regression

```{r}
ridge_fit = glmnet(data.matrix(train.df[,-19]), train.df$y, alpha = 0)

```


iii) Random Forest

```{r}

forest_fit = ranger(y~., data=train.df, importance="impurity")

```



iv) XGBoost

```{r}

xgb_fit = bst <- xgboost(data = data.matrix(train.df), label = train.df$y, nrounds=2, objective = "binary:logistic")

```


*B*
Predictions and confusion Matrices

```{r}
#Logistic Regression
log_pred = predict(log_fit, test.df[,-19])
log_pred = ifelse(log_pred<0.5, 0, 1)
confusionMatrix(as.factor(log_pred), as.factor(test.df$y))

```


```{r}
#Ridge
ridge_pred = predict(ridge_fit, data.matrix(test.df[,-19]))[,100]
ridge_pred = ifelse(ridge_pred<0.5, 0, 1)
confusionMatrix(as.factor(ridge_pred), as.factor(test.df$y))
```

```{r}
#Random Forest
forest_pred = predict(forest_fit, data.matrix(test.df[,-19]))$predictions
forest_pred = ifelse(forest_pred<0.5, 0, 1)
confusionMatrix(as.factor(forest_pred), as.factor(test.df$y))
```

```{r}
#XGBoost
forest_pred = 1
```























