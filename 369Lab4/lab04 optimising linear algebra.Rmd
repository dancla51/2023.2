---
title: 'Lab 4: optimising linear algebra'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r}
#Import libraries and load data
library(leaps)
library(tidyverse)

```

Linear algebra is fundamental for data science. Standard CPUs aren't very good at it: they can spend nearly all the time getting data on and off the CPU and very little time doing productive calculations. Specially designed chips for matrix operations (GPUs) and higher-dimensional tensor operations (TPUs) are the future way around this problem, but optimising data flow is important even there.

The basic idea is to partition a matrix into blocks, chosen to match features of the computer such as cache size.Â  It's hard to derive the optimal block size from first principles, so optimisation is done by measuring many problems of different sizes and trying to fit a model of some sort to find the best settings.

The data (`sgemm_product.csv`) is the result of an exhaustive experiment (`Readme.txt`) in optimising matrix-matrix multiplication for a particular GPU. There are 14 predictor variables in 261400 possible combinations. For each combination there are timings for four runs of multiplying two $2048\times 2048$ matrices. Typically we would not have such exhaustive data, so we want to know how good the prediction from a subsample will be.

You will model `log(time)`, not `time`, because that's what the experiment was designed for. The effects of the variables should be closer to multiplicative than additive.

Some useful code for setting up the design matrix and doing the cross-validation is on Canvas for this lab.

## Tasks

1. Read in the data and choose your own random sample of 500 rows with

```{r}
# Read in data
sgemm_df = read_csv("sgemm_product.csv", show_col_types = FALSE)
# Sample
set.seed(502)
sgemm1<-sgemm_df[sample(241600, 500),]
sgemm2<-sgemm1
```

2. Using the `leaps` package, as in class, run backwards stepwise selection to predict timings from the **logarithm of time** for first run on models with all 14 predictors and all two-way interactions. Look at models with up to 20 variables. Plot the apparent error against the number of predictors. 

Note that you will need a bit of work to get all 2-way interactions in `regsubsets` as you can't use standard model notatation in regsubsets. That is, `regsubsets` requires a premade design matrix with all the interactions. Below in the code chunk below, see two approaches of making a design matrix with all 2-way  interactions.
```{r}
# Create column for log of run1
sgemm1$logrun1 = log(sgemm1$"Run1 (ms)")
sgemm1 = sgemm1 %>% select(!("Run1 (ms)":"Run4 (ms)")) #Remove non-logged run columns

#Do same for run2 for future
sgemm2$logrun2 = log(sgemm2$"Run2 (ms)")
sgemm2 = sgemm2 %>% select(!("Run1 (ms)":"Run4 (ms)")) #Remove non-logged run columns

#Do similar for all observations
sgemm_copy = sgemm_df
sgemm_copy$logrun2 = log(sgemm_copy$"Run2 (ms)")
sgemm_copy = sgemm_copy %>% select(!("Run1 (ms)":"Run4 (ms)")) #Remove non-logged run columns
```


```{r cache=TRUE}
## The easy way: get lm to do it
## assuming sgemm1 is your subset of the data frame with the predictors and logged Run 1 times

model <- lm(logrun1~.^2,data=sgemm1)
X<-model.matrix(model)[,-1]  ## drop the intercept column because regsubsets() will put it back in

## Without actually running lm()
## faster, and still works when there are more variables than observations

mf<-model.frame(logrun1~.^2, data=sgemm1)
X<-model.matrix(logrun1~.^2, mf)[,-1]

rout = regsubsets(X, y=sgemm1$logrun1, nvmax=20, method="back")
rss = summary(rout)$rss

# Plot results
predictors = 1:20
apparent_error <- rss/(100-predictors)
plot(predictors, apparent_error)
```


3. Using cross-validation, select a tuning parameter $\lambda$ so that minimising $n\log\mathrm{RSS}+\lambda p$ gives good mean squared prediction error, and select a predictive model.  

```{r}
# Crossvalidation code
allyhat<-function(xtrain, ytrain, xtest,lambdas,nvmax=50){
  n<-nrow(xtrain)
  yhat<-matrix(nrow=nrow(xtest),ncol=length(lambdas))
  search<-regsubsets(xtrain,ytrain, nvmax=nvmax, method="back")
  summ<-summary(search)
  for(i in 1:length(lambdas)){
    penMSE<- n*log(summ$rss)+lambdas[i]*(1:nvmax)
    best<-which.min(penMSE)  #lowest penMSE
    betahat<-coef(search, best) #coefficients
    xinmodel<-cbind(1,xtest)[,summ$which[best,]] #predictors in that model
    yhat[,i]<-xinmodel%*%betahat
  }
  yhat
}

# Set x and y 's
y = sgemm1$logrun1

# Call using Liza's code
n<-nrow(X)
folds<-sample(rep(1:10,length.out=n))
lambdas<-c(2,4,6,8,10,12)
fitted<-matrix(nrow=n,ncol=length(lambdas))
for(k in 1:10){
  train<- (1:n)[folds!=k]
  test<-(1:n)[folds==k]
  fitted[test,]<-allyhat(X[train,],y[train],X[test,],lambdas)  
}
MSPE = colMeans((y-fitted)^2)
print(MSPE)
```
We can see from the MSPE produced by this function that the most appropriate value of lambda to use is $\lambda=6$ , as this gives the **minimum mean squared prediction error** on our model.


4. Estimate the actual mean squared prediction error of your model using the second replicate of the experiment (`log(Run2)`) in your sample data set.

```{r}
# From slides, edited
# use optimal lambda=6
opt_lambda <- 6

# New X and y
mf<-model.frame(logrun2~.^2, data=sgemm2)   # Check piazza if this is correct X to use!!!!!!!
X<-model.matrix(logrun2~.^2, mf)[,-1]
y = sgemm2$logrun2
  
sub <- regsubsets(X, y, method = 'back', nvmax=50)
summ <- summary(sub)
aic <- 100*log(summ$rss) + opt_lambda*(1:50)
best_model <- which.min(aic)
betahat = coef(sub, best_model)
```

```{r}
# Calculate MSPE
xinmodel<-cbind(1,X)[,summ$which[best_model,]]
yhat<-xinmodel%*%betahat

MSPE_2 = mean((yhat-y)**2)
print(MSPE_2)

```




5. Estimate the actual mean squared prediction error of your model using the second replicate of the experiment (`log(Run2)`) on all 261400 observations. How does this value compare to the estimate you found in question 4?


```{r}

# New X and y
mf<-model.frame(logrun2~.^2, data=sgemm_copy)   # Check piazza if this is correct X to use!!!!!!!
X<-model.matrix(logrun2~.^2, mf)[,-1]
y = sgemm_copy$logrun2
  
sub <- regsubsets(X, y, method = 'back', nvmax=50)
summ <- summary(sub)
aic <- 100*log(summ$rss) + opt_lambda*(1:50)
best_model <- which.min(aic)
betahat = coef(sub, best_model)
```

```{r}
# Calculate MSPE
xinmodel<-cbind(1,X)[,summ$which[best_model,]]
yhat<-xinmodel%*%betahat

MSPE_2 = mean((yhat-y)**2)
print(MSPE_2)

```
This is ***FINISH THIS BIT***


